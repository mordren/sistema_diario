import os
import json
from datetime import datetime
from typing import List, Optional
from enum import Enum
from pydantic import BaseModel, Field
from ddgs import DDGS
import pandas as pd
import time

# Configura√ß√£o da API do Grok
XAI_API_KEY = os.environ.get("XAI_API_KEY")

class GravidadeEnum(str, Enum):
    BAIXA = "baixa"
    MEDIA = "media"
    ALTA = "alta"
    CRITICA = "critica"

class TipoFonteEnum(str, Enum):
    TWITTER = "twitter"
    NOTICIA = "noticia"
    FORUM = "forum"
    BLOG = "blog"
    SITE_OFICIAL = "site_oficial"

class Polemica(BaseModel):
    titulo: str = Field(description="T√≠tulo resumido da pol√™mica")
    descricao: str = Field(description="Descri√ß√£o detalhada da pol√™mica")
    fonte: str = Field(description="URL ou origem da informa√ß√£o")
    tipo_fonte: TipoFonteEnum = Field(description="Tipo da fonte da informa√ß√£o")
    data_incidente: Optional[str] = Field(description="Data do incidente se dispon√≠vel")
    gravidade: GravidadeEnum = Field(description="N√≠vel de gravidade da pol√™mica")
    categorias: List[str] = Field(description="Categorias da pol√™mica")
    evidencias: List[str] = Field(description="Evid√™ncias ou provas mencionadas")
    impacto_publico: str = Field(description="Potencial impacto na opini√£o p√∫blica")
    relevancia: str = Field(description="Relev√¢ncia da informa√ß√£o encontrada")

class AnalisePessoa(BaseModel):
    nome: str = Field(description="Nome completo da pessoa analisada")
    cargo_publico: Optional[str] = Field(description="Cargo p√∫blico se aplic√°vel")
    total_polemicas: int = Field(description="N√∫mero total de pol√™micas encontradas")
    polemicas: List[Polemica] = Field(description="Lista de pol√™micas identificadas")
    resumo_analise: str = Field(description="Resumo geral da an√°lise")
    risco_reputacao: GravidadeEnum = Field(description="Risco geral para reputa√ß√£o")
    data_analise: str = Field(description="Data da an√°lise")
    fontes_consultadas: List[str] = Field(description="Fontes utilizadas na pesquisa")
    tweets_relevantes: List[str] = Field(description="Tweets relevantes encontrados")

class BuscadorTwitterUnificado:
    def __init__(self):
        self.ddgs = DDGS()
        try:
            from xai_sdk import Client
            self.client = Client(api_key=XAI_API_KEY)
            self.grok_available = True
        except ImportError:
            print("‚ö†Ô∏è SDK do Grok n√£o dispon√≠vel")
            self.grok_available = False
    
    def buscar_dados_duckduckgo_completo(self, nome_pessoa):
        """Busca abrangente no DuckDuckGo com m√∫ltiplas queries"""
        print(f"üîç Buscando dados para: {nome_pessoa}")
        
        queries = [
            f'"{nome_pessoa}" twitter pol√™mica',
            f'"{nome_pessoa}" esc√¢ndalo',
            f'"{nome_pessoa}" processo judicial',
            f'"{nome_pessoa}" licita√ß√£o irregular',
            f'"{nome_pessoa}" MPF investiga√ß√£o',
            f'"{nome_pessoa}" condenado',
            f'"{nome_pessoa}" fraude',
            f'"{nome_pessoa}" corrup√ß√£o',
            f'"{nome_pessoa}" improbidade',
            f'"{nome_pessoa}" desvio de verba'
        ]
        
        todos_resultados = []
        
        for i, query in enumerate(queries, 1):
            print(f"  üìù Query {i}/10: {query}")
            try:
                results = list(self.ddgs.text(
                    query=query,
                    region='br-pt',
                    max_results=8
                ))
                todos_resultados.extend(results)
                print(f"    ‚úÖ Encontrados: {len(results)} resultados")
                time.sleep(1.5)  # Rate limiting
            except Exception as e:
                print(f"    ‚ùå Erro na query '{query}': {e}")
                continue
        
        # Remover duplicatas
        resultados_unicos = []
        urls_vistas = set()
        
        for resultado in todos_resultados:
            url = resultado.get('href', '')
            if url and url not in urls_vistas:
                urls_vistas.add(url)
                resultados_unicos.append(resultado)
        
        print(f"üéØ Total de resultados √∫nicos: {len(resultados_unicos)}")
        return resultados_unicos
    
    def analisar_com_grok(self, nome_pessoa, resultados_ddgs):
        """Usa o Grok para analisar os resultados do DuckDuckGo"""
        if not self.grok_available:
            return {"error": "Grok n√£o dispon√≠vel"}
        
        try:
            from xai_sdk.chat import system, user
            
            # Preparar contexto consolidado
            contexto_ddgs = "RESULTADOS CONSOLIDADOS DO DUCKDUCKGO:\n\n"
            for i, resultado in enumerate(resultados_ddgs, 1):
                contexto_ddgs += f"--- RESULTADO {i} ---\n"
                contexto_ddgs += f"T√≠tulo: {resultado.get('title', 'N/A')}\n"
                contexto_ddgs += f"URL: {resultado.get('href', 'N/A')}\n"
                contexto_ddgs += f"Descri√ß√£o: {resultado.get('body', 'N/A')}\n"
                contexto_ddgs += f"Fonte: {self._classificar_fonte_simples(resultado.get('href', ''))}\n\n"
            
            chat = self.client.chat.create(model="grok-2-1212")
            
            prompt = f"""
            ANALISE DE REPUTA√á√ÉO P√öBLICA - {nome_pessoa.upper()}

            BASEADO NOS SEGUINTES RESULTADOS CONSOLIDADOS DE BUSCA:
            {contexto_ddgs}

            ANALISE ESTES RESULTADOS E IDENTIFIQUE:

            üîç POL√äMICAS E CONTROV√âRSIAS:
            - Listar cada pol√™mica encontrada com t√≠tulo descritivo
            - Incluir URL da fonte
            - Classificar gravidade (baixa, media, alta, critica)
            - Categorizar (Licita√ß√µes, Judicial, Eleitoral, etc)

            üìä AN√ÅLISE DE RISCO:
            - Risco geral para reputa√ß√£o
            - Padr√µes de comportamento problem√°tico
            - Impacto potencial na opini√£o p√∫blica

            üéØ DIRETRIZES:
            - Seja objetivo e factual
            - Baseie-se apenas nas informa√ß√µes fornecidas
            - Priorize fontes confi√°veis (sites oficiais, not√≠cias)
            - Inclua tweets apenas quando relevantes como evid√™ncia
            """
            
            chat.append(system("Voc√™ √© um analista especializado em due diligence e an√°lise de reputa√ß√£o p√∫blica com expertise jur√≠dica e pol√≠tica."))
            chat.append(user(prompt))
            
            response, analise = chat.parse(AnalisePessoa)
            
            return analise.dict()
            
        except Exception as e:
            print(f"‚ùå Erro na an√°lise Grok: {e}")
            return {"error": str(e)}
    
    def _classificar_fonte_simples(self, url):
        """Classifica√ß√£o simples da fonte para contexto"""
        url = url.lower()
        if 'twitter.com' in url or 'x.com' in url:
            return "Twitter"
        elif any(site in url for site in ['.gov', '.jus', '.mp', 'tribunal', 'justi√ßa']):
            return "Site Oficial"
        elif any(site in url for site in ['g1', 'uol', 'folha', 'estadao', 'oglobo']):
            return "Not√≠cia"
        else:
            return "Blog/Forum"

class AnalisadorUnificado:
    def __init__(self):
        self.buscador = BuscadorTwitterUnificado()
    
    def analisar_pessoa(self, nome_pessoa, cargo_publico=None):
        """Fluxo unificado: DuckDuckGo -> Grok -> An√°lise"""
        print(f"\nüéØ INICIANDO AN√ÅLISE: {nome_pessoa}")
        print("=" * 60)
        
        # Fase 1: Busca consolidada no DuckDuckGo
        print("\nüîç FASE 1: BUSCA CONSOLIDADA DUCKDUCKGO...")
        resultados_ddgs = self.buscador.buscar_dados_duckduckgo_completo(nome_pessoa)
        
        if not resultados_ddgs:
            print("‚ùå Nenhum resultado encontrado no DuckDuckGo")
            return self._criar_analise_vazia(nome_pessoa, cargo_publico)
        
        # Salvar resultados brutos
        self._salvar_resultados_brutos(resultados_ddgs, nome_pessoa)
        
        # Fase 2: An√°lise com Grok
        print("\nü§ñ FASE 2: AN√ÅLISE COM GROK...")
        analise_grok = self.buscador.analisar_com_grok(nome_pessoa, resultados_ddgs)
        
        # Fase 3: Consolida√ß√£o final
        print("\nüìä FASE 3: CONSOLIDA√á√ÉO DOS RESULTADOS...")
        analise_final = self._processar_analise_final(analise_grok, resultados_ddgs, nome_pessoa, cargo_publico)
        
        # Fase 4: Salvar e reportar
        print("\nüíæ FASE 4: SALVANDO RESULTADOS...")
        self._salvar_analise_completa(analise_final, nome_pessoa)
        
        return analise_final
    
    def _processar_analise_final(self, analise_grok, resultados_ddgs, nome_pessoa, cargo_publico):
        """Processa e consolida a an√°lise final"""
        
        # Se Grok falhou, criar an√°lise b√°sica com DuckDuckGo
        if "error" in analise_grok:
            print("‚ö†Ô∏è Usando fallback DuckDuckGo (Grok indispon√≠vel)")
            return self._criar_analise_ddgs(resultados_ddgs, nome_pessoa, cargo_publico)
        
        # Enriquecer an√°lise do Grok com dados do DuckDuckGo
        analise_grok['fontes_consultadas'].append("DuckDuckGo (Busca Consolidada)")
        
        # Extrair tweets relevantes dos resultados
        tweets = []
        for resultado in resultados_ddgs:
            url = resultado.get('href', '')
            if 'twitter.com' in url or 'x.com' in url:
                tweet_info = {
                    'texto': resultado.get('title', ''),
                    'url': url,
                    'descricao': resultado.get('body', '')[:150]
                }
                tweets.append(json.dumps(tweet_info, ensure_ascii=False))
        
        if tweets:
            analise_grok['tweets_relevantes'] = tweets
        
        analise_grok['data_analise'] = datetime.now().isoformat()
        
        return analise_grok
    
    def _criar_analise_ddgs(self, resultados_ddgs, nome_pessoa, cargo_publico):
        """Cria an√°lise baseada apenas no DuckDuckGo"""
        polemicas = []
        
        for resultado in resultados_ddgs:
            polemica = Polemica(
                titulo=resultado.get('title', 'Sem t√≠tulo')[:100],
                descricao=resultado.get('body', 'Sem descri√ß√£o')[:200],
                fonte=resultado.get('href', ''),
                tipo_fonte=self._classificar_fonte(resultado.get('href', '')),
                gravidade=self._classificar_gravidade(resultado.get('title', '') + resultado.get('body', '')),
                categorias=self._extrair_categorias(resultado.get('title', '') + resultado.get('body', '')),
                evidencias=[resultado.get('body', '')[:100]],
                impacto_publico="A ser avaliado",
                relevancia="M√©dia"
            )
            polemicas.append(polemica.dict())
        
        analise = AnalisePessoa(
            nome=nome_pessoa,
            cargo_publico=cargo_publico,
            total_polemicas=len(polemicas),
            polemicas=polemicas,
            resumo_analise="An√°lise baseada em busca DuckDuckGo - Grok indispon√≠vel",
            risco_reputacao=self._calcular_risco_geral(polemicas),
            data_analise=datetime.now().isoformat(),
            fontes_consultadas=["DuckDuckGo (Busca Consolidada)"],
            tweets_relevantes=[]
        )
        
        return analise.dict()
    
    def _criar_analise_vazia(self, nome_pessoa, cargo_publico):
        """Cria an√°lise vazia quando n√£o h√° resultados"""
        analise = AnalisePessoa(
            nome=nome_pessoa,
            cargo_publico=cargo_publico,
            total_polemicas=0,
            polemicas=[],
            resumo_analise="Nenhuma pol√™mica encontrada nas buscas realizadas",
            risco_reputacao=GravidadeEnum.BAIXA,
            data_analise=datetime.now().isoformat(),
            fontes_consultadas=["DuckDuckGo"],
            tweets_relevantes=[]
        )
        return analise.dict()
    
    def _classificar_fonte(self, url):
        url = url.lower()
        if 'twitter.com' in url or 'x.com' in url:
            return TipoFonteEnum.TWITTER
        elif any(site in url for site in ['.gov', '.jus', '.mp']):
            return TipoFonteEnum.SITE_OFICIAL
        elif any(site in url for site in ['.com.br', '.com']):
            return TipoFonteEnum.NOTICIA
        else:
            return TipoFonteEnum.BLOG
    
    def _classificar_gravidade(self, texto):
        texto = texto.lower()
        if any(termo in texto for termo in ['corrup√ß√£o', 'condenado', 'pris√£o', 'desvio', 'crime']):
            return GravidadeEnum.CRITICA
        elif any(termo in texto for termo in ['investiga√ß√£o', 'processo', 'den√∫ncia', 'improbidade', 'fraude']):
            return GravidadeEnum.ALTA
        elif any(termo in texto for termo in ['pol√™mica', 'controv√©rsia', 'cr√≠tica', 'questionamento']):
            return GravidadeEnum.MEDIA
        else:
            return GravidadeEnum.BAIXA
    
    def _extrair_categorias(self, texto):
        categorias = []
        texto = texto.lower()
        
        if any(termo in texto for termo in ['licita√ß√£o', 'contrato', 'preg√£o']):
            categorias.append("Licita√ß√µes")
        if any(termo in texto for termo in ['elei√ß√£o', 'campanha', 'doa√ß√£o']):
            categorias.append("Eleitoral")
        if any(termo in texto for termo in ['corrup√ß√£o', 'desvio', 'propina']):
            categorias.append("Corrup√ß√£o")
        if any(termo in texto for termo in ['processo', 'judicial', 'tribunal']):
            categorias.append("Judicial")
            
        return categorias if categorias else ["Outros"]
    
    def _calcular_risco_geral(self, polemicas):
        if not polemicas:
            return GravidadeEnum.BAIXA
        
        gravidades = [p.get('gravidade', 'baixa') for p in polemicas]
        if any(g == 'critica' for g in gravidades):
            return GravidadeEnum.CRITICA
        elif any(g == 'alta' for g in gravidades):
            return GravidadeEnum.ALTA
        elif any(g == 'media' for g in gravidades):
            return GravidadeEnum.MEDIA
        else:
            return GravidadeEnum.BAIXA
    
    def _salvar_resultados_brutos(self, resultados_ddgs, nome_pessoa):
        """Salva resultados brutos do DuckDuckGo"""
        arquivo_bruto = f"resultados_brutos_{nome_pessoa.replace(' ', '_')}.json"
        dados_brutos = {
            'nome_pessoa': nome_pessoa,
            'data_busca': datetime.now().isoformat(),
            'total_resultados': len(resultados_ddgs),
            'resultados': resultados_ddgs
        }
        
        with open(arquivo_bruto, 'w', encoding='utf-8') as f:
            json.dump(dados_brutos, f, ensure_ascii=False, indent=2)
        
        print(f"üíæ Dados brutos salvos: {arquivo_bruto}")
    
    def _salvar_analise_completa(self, analise, nome_pessoa):
        """Salva an√°lise completa em JSON"""
        arquivo = f"analise_completa_{nome_pessoa.replace(' ', '_')}.json"
        
        with open(arquivo, 'w', encoding='utf-8') as f:
            json.dump(analise, f, ensure_ascii=False, indent=2)
        
        print(f"‚úÖ An√°lise salva: {arquivo}")
        
        # Gerar relat√≥rio resumido
        self._gerar_relatorio_console(analise)
    
    def _gerar_relatorio_console(self, analise):
        """Gera relat√≥rio resumido no console"""
        print(f"\nüìä RELAT√ìRIO FINAL - {analise['nome']}")
        print("=" * 50)
        print(f"üîç Pol√™micas encontradas: {analise['total_polemicas']}")
        print(f"üö® Risco reputa√ß√£o: {analise['risco_reputacao'].upper()}")
        print(f"üìÖ Data an√°lise: {analise['data_analise'][:10]}")
        print(f"üîß Fontes: {', '.join(analise['fontes_consultadas'])}")
        
        if analise.get('tweets_relevantes'):
            print(f"üê¶ Tweets relevantes: {len(analise['tweets_relevantes'])}")
        
        if analise['polemicas']:
            print(f"\nüéØ PRINCIPAIS POL√äMICAS:")
            for i, polemica in enumerate(analise['polemicas'][:3], 1):
                print(f"\n{i}. {polemica['titulo']}")
                print(f"   üìç Gravidade: {polemica['gravidade'].upper()}")
                print(f"   üìù {polemica['descricao'][:100]}...")
                print(f"   üîó Fonte: {polemica['tipo_fonte']}")

def executar_analise(nome_pessoa, cargo=None):
    """Fun√ß√£o principal para executar an√°lise"""
    analisador = AnalisadorUnificado()
    
    print(f"\n{'#'*60}")
    print(f"üöÄ INICIANDO AN√ÅLISE UNIFICADA: {nome_pessoa}")
    print(f"{'#'*60}")
    
    try:
        inicio = time.time()
        analise = analisador.analisar_pessoa(nome_pessoa, cargo)
        tempo_total = time.time() - inicio
        
        print(f"\n‚úÖ AN√ÅLISE CONCLU√çDA em {tempo_total:.1f} segundos")
        return analise
        
    except Exception as e:
        print(f"‚ùå Erro na an√°lise: {e}")
        return None

# EXECU√á√ÉO PRINCIPAL
if __name__ == "__main__":
    nome = "SANDRO ALEX CRUZ DE OLIVEIRA"  # üîß ALTERE AQUI
    cargo = "Trabalha no governo estadual do paran√°"  # üîß OPCIONAL
    
    resultado = executar_analise(nome, cargo)